{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeadbeatJeff/UTIRnet/blob/main/hologram_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQDavi5c--jA",
        "outputId": "462ffd63-a153-486e-ff89-c131449256e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!micromamba install matplotlib scipy scikit-image -y"
      ],
      "metadata": {
        "id": "nyBbEBRt5f0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59a5e918-2bb5-4d3c-90b9-39f3d88729c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conda-forge/linux-64                                        Using cache\r\n",
            "conda-forge/noarch                                          Using cache\n",
            "\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\n",
            "nodefaults/linux-64 \u001b[90m━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\n",
            "nodefaults/noarch   \u001b[90m━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.2s\n",
            "nodefaults/linux-64 \u001b[90m━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\n",
            "nodefaults/noarch   \u001b[90m━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.3s\n",
            "nodefaults/linux-64 \u001b[90m━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.2s\n",
            "nodefaults/noarch   \u001b[90m━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.2s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gnodefaults/linux-64                                125.0 B @ 350.0 B/s  0.3s\n",
            "nodefaults/noarch                                  116.0 B @ 325.0 B/s  0.3s\n",
            "\u001b[?25h\n",
            "Pinned packages:\n",
            "\n",
            "  - python=3.11\n",
            "\n",
            "\n",
            "Transaction\n",
            "\n",
            "  Prefix: /home/tomh/micromamba\n",
            "\n",
            "  Updating specs:\n",
            "\n",
            "   - matplotlib\n",
            "   - scipy\n",
            "   - scikit-image\n",
            "\n",
            "\n",
            "  Package              Version  Build            Channel           Size\n",
            "─────────────────────────────────────────────────────────────────────────\n",
            "  Install:\n",
            "─────────────────────────────────────────────────────────────────────────\n",
            "\n",
            "  \u001b[32m+ aom            \u001b[0m      3.5.0  h27087fc_0       conda-forge        3MB\n",
            "  \u001b[32m+ blosc          \u001b[0m     1.21.5  h0f2a231_0       conda-forge       49kB\n",
            "  \u001b[32m+ brunsli        \u001b[0m        0.1  h9c3ff4c_0       conda-forge      205kB\n",
            "  \u001b[32m+ c-ares         \u001b[0m     1.34.5  hb9d3cd8_0       conda-forge      207kB\n",
            "  \u001b[32m+ c-blosc2       \u001b[0m     2.12.0  hb4ffafa_0       conda-forge      334kB\n",
            "  \u001b[32m+ cfitsio        \u001b[0m      4.2.0  hd9d235c_0       conda-forge      848kB\n",
            "  \u001b[32m+ charls         \u001b[0m      2.4.2  h59595ed_0       conda-forge      150kB\n",
            "  \u001b[32m+ dav1d          \u001b[0m      1.2.1  hd590300_0       conda-forge      760kB\n",
            "  \u001b[32m+ imagecodecs    \u001b[0m  2023.1.23  py311ha5a3c35_0  conda-forge        2MB\n",
            "  \u001b[32m+ imageio        \u001b[0m     2.37.0  pyhfb79c49_0     conda-forge      293kB\n",
            "  \u001b[32m+ jxrlib         \u001b[0m        1.1  hd590300_3       conda-forge      239kB\n",
            "  \u001b[32m+ lazy-loader    \u001b[0m        0.4  pyhd8ed1ab_2     conda-forge       16kB\n",
            "  \u001b[32m+ lazy_loader    \u001b[0m        0.4  pyhd8ed1ab_2     conda-forge        7kB\n",
            "  \u001b[32m+ libaec         \u001b[0m      1.1.4  h3f801dc_0       conda-forge       37kB\n",
            "  \u001b[32m+ libavif        \u001b[0m     0.11.1  h8182462_2       conda-forge      102kB\n",
            "  \u001b[32m+ libcurl        \u001b[0m      8.1.2  h409715c_0       conda-forge      373kB\n",
            "  \u001b[32m+ libev          \u001b[0m       4.33  hd590300_2       conda-forge      113kB\n",
            "  \u001b[32m+ libnghttp2     \u001b[0m     1.58.0  h47da74e_0       conda-forge      631kB\n",
            "  \u001b[32m+ libssh2        \u001b[0m     1.11.0  h0841786_0       conda-forge      271kB\n",
            "  \u001b[32m+ libzopfli      \u001b[0m      1.0.3  h9c3ff4c_0       conda-forge      168kB\n",
            "  \u001b[32m+ pywavelets     \u001b[0m      1.9.0  py311h0372a8f_1  conda-forge        4MB\n",
            "  \u001b[32m+ scikit-image   \u001b[0m     0.25.0  py311h7db5c69_0  conda-forge       11MB\n",
            "  \u001b[32m+ snappy         \u001b[0m     1.1.10  hdb0a2a9_1       conda-forge       40kB\n",
            "  \u001b[32m+ tifffile       \u001b[0m  2023.8.12  pyhd8ed1ab_0     conda-forge      175kB\n",
            "  \u001b[32m+ zfp            \u001b[0m      1.0.1  h909a3a2_3       conda-forge      277kB\n",
            "  \u001b[32m+ zlib-ng        \u001b[0m      2.0.7  h0b41bf4_0       conda-forge       95kB\n",
            "\n",
            "  Downgrade:\n",
            "─────────────────────────────────────────────────────────────────────────\n",
            "\n",
            "  \u001b[31m- brotli         \u001b[0m      1.1.0  hb03c661_4       conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ brotli         \u001b[0m      1.0.9  h166bdaf_9       conda-forge       20kB\n",
            "  \u001b[31m- brotli-bin     \u001b[0m      1.1.0  hb03c661_4       conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ brotli-bin     \u001b[0m      1.0.9  h166bdaf_9       conda-forge       20kB\n",
            "  \u001b[31m- brotli-python  \u001b[0m      1.1.0  py311h1ddb823_4  conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ brotli-python  \u001b[0m      1.0.9  py311ha362b79_9  conda-forge      327kB\n",
            "  \u001b[31m- libbrotlicommon\u001b[0m      1.1.0  hb03c661_4       conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ libbrotlicommon\u001b[0m      1.0.9  h166bdaf_9       conda-forge       71kB\n",
            "  \u001b[31m- libbrotlidec   \u001b[0m      1.1.0  hb03c661_4       conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ libbrotlidec   \u001b[0m      1.0.9  h166bdaf_9       conda-forge       33kB\n",
            "  \u001b[31m- libbrotlienc   \u001b[0m      1.1.0  hb03c661_4       conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ libbrotlienc   \u001b[0m      1.0.9  h166bdaf_9       conda-forge      265kB\n",
            "  \u001b[31m- numpy          \u001b[0m      2.3.3  py311h2e04523_0  conda-forge\u001b[32m     Cached\u001b[0m\n",
            "  \u001b[32m+ numpy          \u001b[0m     1.26.4  py311h64a7726_0  conda-forge        8MB\n",
            "\n",
            "  Summary:\n",
            "\n",
            "  Install: 26 packages\n",
            "  Downgrade: 7 packages\n",
            "\n",
            "  Total download: 34MB\n",
            "\n",
            "─────────────────────────────────────────────────────────────────────────\n",
            "\n",
            "\n",
            "\n",
            "Transaction starting\n",
            "\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
            "Downloading      ╸\u001b[33m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   0.0 B                            0.0s\n",
            "Extracting       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m       0                            0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\n",
            "Downloading  (5) \u001b[33m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   0.0 B aom                        0.0s\n",
            "Extracting       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m       0                            0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.2s\n",
            "Downloading  (5) \u001b[33m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 740.2kB aom                        0.1s\n",
            "Extracting       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m       0                            0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.3s\n",
            "Downloading  (5) ━━━━╸\u001b[33m━━━━━━━━━━━━━━━━━━\u001b[0m   6.7MB aom                        0.2s\n",
            "Extracting       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m       0                            0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gaom                                                  2.9MB @   8.1MB/s  0.3s\n",
            "pywavelets                                           3.7MB @   8.0MB/s  0.3s\n",
            "[+] 0.4s\n",
            "Downloading  (5) ━━━━━━━━━╸\u001b[33m━━━━━━━━━━━━━\u001b[0m  13.9MB cfitsio                    0.3s\n",
            "Extracting   (2) \u001b[33m━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m       0 aom                        0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gcfitsio                                            848.0kB @  ??.?MB/s  0.1s\n",
            "dav1d                                              760.2kB @  ??.?MB/s  0.1s\n",
            "[+] 0.5s\n",
            "Downloading  (5) ━━━━━━━━━━━━━╸\u001b[33m━━━━━━━━━\u001b[0m  19.3MB imagecodecs                0.4s\n",
            "Extracting   (1) ━╸\u001b[33m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m       3 aom                        0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gimagecodecs                                          2.0MB @   3.9MB/s  0.5s\n",
            "libnghttp2                                         631.4kB @  ??.?MB/s  0.1s\n",
            "libcurl                                            372.8kB @  ??.?MB/s  0.1s\n",
            "numpy                                                8.1MB @  14.2MB/s  0.5s\n",
            "brotli-python                                      327.1kB @  ??.?MB/s  0.0s\n",
            "imageio                                            293.2kB @   4.1MB/s  0.1s\n",
            "[+] 0.6s\n",
            "Downloading  (5) ━━━━━━━━━━━━━━━━━╸\u001b[33m━━━━━\u001b[0m  25.7MB c-blosc2                   0.5s\n",
            "Extracting   (4) ━━━╸\u001b[33m━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m       6 aom                        0.2s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gc-blosc2                                           333.7kB @  ??.?MB/s  0.1s\n",
            "zfp                                                277.4kB @   1.7MB/s  0.1s\n",
            "libbrotlienc                                       265.2kB @  ??.?MB/s  0.1s\n",
            "libssh2                                            271.1kB @  ??.?MB/s  0.1s\n",
            "jxrlib                                             239.1kB @  ??.?MB/s  0.1s\n",
            "[+] 0.7s\n",
            "Downloading  (5) ━━━━━━━━━━━━━━━━━━━╸\u001b[33m━━━\u001b[0m  28.9MB brunsli                    0.6s\n",
            "Extracting   (1) ━━━━━━━━╸\u001b[33m━╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m      14 jxrlib                     0.3s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gc-ares                                             206.9kB @  ??.?MB/s  0.1s\n",
            "tifffile                                           175.0kB @  ??.?MB/s  0.1s\n",
            "brunsli                                            204.9kB @  ??.?MB/s  0.1s\n",
            "charls                                             150.3kB @  ??.?MB/s  0.1s\n",
            "libzopfli                                          168.1kB @  ??.?MB/s  0.1s\n",
            "libavif                                            102.2kB @   1.1MB/s  0.1s\n",
            "libev                                              112.8kB @  ??.?MB/s  0.1s\n",
            "[+] 0.8s\n",
            "Downloading  (5) ━━━━━━━━━━━━━━━━━━━━╸\u001b[33m━━\u001b[0m  32.2MB blosc                      0.7s\n",
            "Extracting       ━━━━━━━━━━━━━━╸\u001b[90m━━━━━━━━\u001b[0m      22                            0.4s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Glibbrotlicommon                                     71.1kB @   1.0MB/s  0.1s\n",
            "scikit-image                                        11.2MB @  13.5MB/s  0.8s\n",
            "snappy                                              40.1kB @ 802.1kB/s  0.1s\n",
            "blosc                                               48.7kB @  ??.?MB/s  0.1s\n",
            "zlib-ng                                             94.6kB @  ??.?MB/s  0.1s\n",
            "libaec                                              36.8kB @  ??.?MB/s  0.0s\n",
            "libbrotlidec                                        32.6kB @  ??.?MB/s  0.0s\n",
            "brotli                                              20.1kB @ 387.1kB/s  0.1s\n",
            "brotli-bin                                          20.4kB @  ??.?MB/s  0.1s\n",
            "[+] 0.9s\n",
            "Downloading  (2) ━━━━━━━━━━━━━━━━━━━━━╸\u001b[33m━\u001b[0m  34.0MB lazy-loader                0.8s\n",
            "Extracting   (2) ━━━━━━━━━━━━━━━━━━━╸\u001b[33m━╸\u001b[0m\u001b[90m━\u001b[0m      29 brotli-bin                 0.4s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Glazy-loader                                         16.3kB @ 285.2kB/s  0.1s\n",
            "lazy_loader                                          6.7kB @  ??.?MB/s  0.0s\n",
            "\u001b[?25hUnlinking libbrotlicommon-1.1.0-hb03c661_4\n",
            "Unlinking brotli-python-1.1.0-py311h1ddb823_4\n",
            "Unlinking numpy-2.3.3-py311h2e04523_0\n",
            "Unlinking libbrotlidec-1.1.0-hb03c661_4\n",
            "Unlinking libbrotlienc-1.1.0-hb03c661_4\n",
            "Unlinking brotli-bin-1.1.0-hb03c661_4\n",
            "Unlinking brotli-1.1.0-hb03c661_4\n",
            "Linking libbrotlicommon-1.0.9-h166bdaf_9\n",
            "Linking brotli-python-1.0.9-py311ha362b79_9\n",
            "Linking numpy-1.26.4-py311h64a7726_0\n",
            "Linking zlib-ng-2.0.7-h0b41bf4_0\n",
            "Linking charls-2.4.2-h59595ed_0\n",
            "Linking jxrlib-1.1-hd590300_3\n",
            "Linking libaec-1.1.4-h3f801dc_0\n",
            "Linking dav1d-1.2.1-hd590300_0\n",
            "Linking aom-3.5.0-h27087fc_0\n",
            "Linking snappy-1.1.10-hdb0a2a9_1\n",
            "Linking zfp-1.0.1-h909a3a2_3\n",
            "Linking libssh2-1.11.0-h0841786_0\n",
            "Linking c-ares-1.34.5-hb9d3cd8_0\n",
            "Linking libev-4.33-hd590300_2\n",
            "Linking libzopfli-1.0.3-h9c3ff4c_0\n",
            "Linking libbrotlidec-1.0.9-h166bdaf_9\n",
            "Linking libbrotlienc-1.0.9-h166bdaf_9\n",
            "Linking pywavelets-1.9.0-py311h0372a8f_1\n",
            "Linking c-blosc2-2.12.0-hb4ffafa_0\n",
            "Linking libavif-0.11.1-h8182462_2\n",
            "Linking blosc-1.21.5-h0f2a231_0\n",
            "Linking libnghttp2-1.58.0-h47da74e_0\n",
            "Linking brotli-bin-1.0.9-h166bdaf_9\n",
            "Linking libcurl-8.1.2-h409715c_0\n",
            "Linking brotli-1.0.9-h166bdaf_9\n",
            "Linking cfitsio-4.2.0-hd9d235c_0\n",
            "Linking brunsli-0.1-h9c3ff4c_0\n",
            "Linking imagecodecs-2023.1.23-py311ha5a3c35_0\n",
            "Linking lazy-loader-0.4-pyhd8ed1ab_2\n",
            "Linking imageio-2.37.0-pyhfb79c49_0\n",
            "Linking tifffile-2023.8.12-pyhd8ed1ab_0\n",
            "Linking lazy_loader-0.4-pyhd8ed1ab_2\n",
            "Linking scikit-image-0.25.0-py311h7db5c69_0\n",
            "\n",
            "Transaction finished\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.fft import fft2, ifft2, fftshift\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from skimage.transform import resize\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import datetime # Import the datetime module\n",
        "\n",
        "# Mock dimensions: 512x512 image size, 1 channel, 500 total samples\n",
        "H, W = 512, 512\n",
        "N_TRAIN, N_VAL = 500, 100\n",
        "IMG_SHAPE = (1, H, W)\n",
        "\n",
        "# ==================================\n",
        "# 0. NEW CUSTOM UTILITY FUNCTIONS\n",
        "# ==================================\n",
        "\n",
        "def torchdevice():\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 1. CORE UTILITY FUNCTIONS (Converted from AS_propagate_p.m, GenerateHologram.m)\n",
        "# =====================================================================\n",
        "\n",
        "def mat2gray(img):\n",
        "    \"\"\"Equivalent to MATLAB's mat2gray. Scales the data in the array to the range [0, 1].\"\"\"\n",
        "    min_val = np.min(img)\n",
        "    max_val = np.max(img)\n",
        "    if max_val == min_val:\n",
        "        return np.zeros_like(img)\n",
        "    return (img - min_val) / (max_val - min_val)\n",
        "\n",
        "def padarray(img, pad_size, method='edge'):\n",
        "    \"\"\"Simplified equivalent of MATLAB's padarray with 'replicate'/'edge' mode.\"\"\"\n",
        "    rows_pad, cols_pad = pad_size\n",
        "    return np.pad(img, ((rows_pad, rows_pad), (cols_pad, cols_pad)), mode=method)\n",
        "\n",
        "def BM3D_(img, sigma):\n",
        "    \"\"\"PLACEHOLDER for the optional BM3D denoising function.\"\"\"\n",
        "    print(\"WARNING: Using BM3D_ PLACEHOLDER. Replace with actual function if needed.\")\n",
        "    return img\n",
        "\n",
        "def AS_propagate_p(uin, z, n0, lambda_val, dx):\n",
        "    \"\"\"Angular Spectrum propagation (Converted from AS_propagate_p.m).\"\"\"\n",
        "\n",
        "    Ny, Nx = uin.shape\n",
        "    k = 2 * np.pi / lambda_val\n",
        "\n",
        "    # Frequency coordinates (standard unshifted order)\n",
        "    fx = np.fft.fftfreq(Nx, d=dx)\n",
        "    fy = np.fft.fftfreq(Ny, d=dx)\n",
        "\n",
        "    FX, FY = np.meshgrid(fx, fy)\n",
        "    R_squared = FX**2 + FY**2\n",
        "\n",
        "    # Propagation exponent term 'p' calculation\n",
        "    # Ensure the term inside the square root is treated as complex before sqrt\n",
        "    sqrt_term_inside = n0**2 - lambda_val**2 * R_squared\n",
        "    p = k * z * np.sqrt(sqrt_term_inside.astype(complex)) # Explicitly cast to complex\n",
        "\n",
        "\n",
        "    # Apply FFT-shift and the subtraction of the wrap-around phase.\n",
        "    p_shifted = np.fft.fftshift(p)\n",
        "    # The corner element p_shifted[0, 0] is the highest frequency component after shift.\n",
        "    p_shifted = p_shifted - p_shifted[0, 0]\n",
        "\n",
        "    if z < 0:\n",
        "        # Back propagation case (z < 0)\n",
        "        kernel = np.exp(-1j * p_shifted)\n",
        "        ftu = kernel * np.fft.fft2(np.conj(uin))\n",
        "        uout = np.conj(np.fft.ifft2(ftu))\n",
        "    else:\n",
        "        # Forward propagation case (z >= 0)\n",
        "        kernel = np.exp(1j * p_shifted)\n",
        "        ftu = kernel * np.fft.fft2(uin)\n",
        "        uout = np.fft.ifft2(ftu)\n",
        "\n",
        "    return uout\n",
        "\n",
        "def GenerateHologram(img, Z, lambda_val, dx, type_val):\n",
        "    \"\"\"Function for generating input and target images (Converted from GenerateHologram.m).\"\"\"\n",
        "\n",
        "    RI = 1\n",
        "    imS = 512\n",
        "    padS = imS // 2\n",
        "    den = 0\n",
        "    xi = 5\n",
        "    phi = (np.random.rand() * 1.5 + 0.5) * np.pi\n",
        "    sigma = 10\n",
        "\n",
        "    # Preprocessing\n",
        "    if img.ndim == 3:\n",
        "        img = np.mean(img, axis=2)\n",
        "    img = img.astype(np.float64)\n",
        "    img = mat2gray(img)\n",
        "\n",
        "    if np.mean(img) < 0.5:\n",
        "        img = 1 - img\n",
        "\n",
        "    img = resize(img, (imS, imS), anti_aliasing=True)\n",
        "    img = img * (1 + xi / 100)\n",
        "    img[img > 1] = 1\n",
        "\n",
        "    if den == 1:\n",
        "        img = BM3D_(img, 20)\n",
        "\n",
        "    # preprocessing for phase objects\n",
        "    if type_val == 'phs':\n",
        "        img = img - gaussian_filter(img, sigma)\n",
        "        img[img > 0] = 0\n",
        "        img = (mat2gray(img) - 1) * phi\n",
        "        img_target = np.angle(np.exp(1j * img)) + np.pi\n",
        "    else: # 'amp'\n",
        "        img_target = img\n",
        "\n",
        "    # pad object\n",
        "    imgP = padarray(img, (padS, padS), 'edge')\n",
        "    if type_val == 'phs':\n",
        "        u0 = np.exp(1j * imgP)\n",
        "    else: # 'amp'\n",
        "        u0 = imgP\n",
        "\n",
        "    # propagate object to camera plane\n",
        "    u_in = AS_propagate_p(u0, -Z, RI, lambda_val, dx)\n",
        "    holo = np.abs(u_in[padS : padS + imS, padS : padS + imS]) ** 2\n",
        "\n",
        "    # Backpropagation to create the CNN input (with twin-image)\n",
        "    if type_val == 'phs':\n",
        "        u_out = AS_propagate_p(np.abs(u_in)**2, Z, RI, lambda_val, dx)\n",
        "        u_out = u_out[padS : padS + imS, padS : padS + imS]\n",
        "        img_input = np.angle(u_out) + np.pi\n",
        "    else: # 'amp'\n",
        "        u_out = AS_propagate_p(np.abs(u_in), Z, RI, lambda_val, dx)\n",
        "        u_out = u_out[padS : padS + imS, padS : padS + imS]\n",
        "        img_input = np.abs(u_out)\n",
        "\n",
        "    return img_input, img_target, holo\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 2. UTIRnet Model Definition (U-Net Architecture)\n",
        "# =====================================================================\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => ReLU) * 2 block used in U-Net\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling block: MaxPool then DoubleConv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling block: ConvTranspose2d, Concatenation with skip, then DoubleConv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, skip_channels):\n",
        "        super().__init__()\n",
        "        # The ConvTranspose2d takes in_channels and outputs out_channels\n",
        "        # out_channels should be the same as skip_channels for concatenation\n",
        "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "        # The DoubleConv layer takes the concatenated input (upsampled + skip)\n",
        "        # Input channels to DoubleConv = output channels of up + skip connection channels\n",
        "        self.conv = DoubleConv(out_channels + skip_channels, out_channels)\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # x1 is the input from the lower level (will be upsampled)\n",
        "        # x2 is the skip connection from the corresponding encoder level\n",
        "        x1 = self.up(x1)\n",
        "\n",
        "        # Padding to handle spatial size differences (common in U-Net implementations)\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "\n",
        "        # Concatenate x2 (skip connection) and x1 (upsampled features) along the channel dimension\n",
        "\n",
        "        x = torch.cat([x2, x1], dim=1).to(torchdevice())\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class UTIRnetComponent(nn.Module):\n",
        "    \"\"\"\n",
        "    UTIRnet Component (CNN_A or CNN_P) implemented as a 5-level U-Net.\n",
        "    Input/Output: 1 channel (Amplitude or Phase).\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(UTIRnetComponent, self).__init__()\n",
        "\n",
        "        # Initial Convolution (Input: 1 channel)\n",
        "        self.inc = DoubleConv(1, 64)\n",
        "\n",
        "        # Encoder (Downsampling path)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 512) # Bottom layer\n",
        "\n",
        "        # Decoder (Upsampling path)\n",
        "        # Up1: takes 512 from down4, outputs 512 to match skip x4 (512 channels)\n",
        "        self.up1 = Up(512, 512, 512)\n",
        "        # Up2: takes 512 from up1, outputs 256 to match skip x3 (256 channels)\n",
        "        self.up2 = Up(512, 256, 256)\n",
        "        # Up3: takes 256 from up2, outputs 128 to match skip x2 (128 channels)\n",
        "        self.up3 = Up(256, 128, 128)\n",
        "        # Up4: takes 128 from up3, outputs 64 to match skip x1 (64 channels)\n",
        "        self.up4 = Up(128, 64, 64)\n",
        "\n",
        "\n",
        "        # Output Convolution (Output: 1 channel)\n",
        "        self.outc = nn.Conv2d(64, 1, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder Path\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4) # Bottom\n",
        "\n",
        "        # Decoder Path (with skip connections)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "\n",
        "        # Final Output\n",
        "        logits = self.outc(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 3. Piecewise Learning Rate Scheduler (MATLAB Equivalent)\n",
        "# =====================================================================\n",
        "\n",
        "class PiecewiseScheduler(optim.lr_scheduler._LRScheduler):\n",
        "    \"\"\"\n",
        "    Custom scheduler implementing the MATLAB 'piecewise' logic:\n",
        "    Learning rate drops by 'factor' every 'step_size' epochs.\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, step_size, factor, last_epoch=-1):\n",
        "        self.step_size = step_size\n",
        "        self.factor = factor\n",
        "        super().__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        # Calculate the number of steps completed\n",
        "        steps = self.last_epoch // self.step_size\n",
        "        # Apply the factor for the current step\n",
        "        return [base_lr * (self.factor ** steps) for base_lr in self.base_lrs]\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 4. Core Training Function\n",
        "# =====================================================================\n",
        "\n",
        "def train_network(model, params, inputs_train, targets_train, inputs_val, targets_val, component_name):\n",
        "    \"\"\"\n",
        "    Implements the PyTorch training loop based on the MATLAB parameters.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Training {component_name} ---\")\n",
        "\n",
        "    # 1. Data Loaders\n",
        "    # You need to define a PyTorch Dataset class for your data\n",
        "    # For now, assuming inputs_train/targets_train are already numpy arrays\n",
        "    # and need to be converted to tensors.\n",
        "    # NOTE: This is a placeholder; replace with a proper Dataset if needed.\n",
        "    class HolographyDataset(Dataset):\n",
        "        def __init__(self, inputs, targets):\n",
        "            # Convert numpy arrays to PyTorch tensors\n",
        "            # Assuming inputs/targets are (N, H, W, 1)\n",
        "            # Permute to (N, 1, H, W) for PyTorch Conv2d input format\n",
        "            self.inputs = torch.from_numpy(inputs).permute(0, 3, 1, 2).float().cuda()\n",
        "            self.targets = torch.from_numpy(targets).permute(0, 3, 1, 2).float().cuda()\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.inputs)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            return self.inputs[idx], self.targets[idx]\n",
        "\n",
        "    train_dataset = HolographyDataset(inputs_train, targets_train)\n",
        "    val_dataset = HolographyDataset(inputs_val, targets_val)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=params['MiniBatchSize'],\n",
        "        shuffle=True,\n",
        "        num_workers=0 # Set to > 0 for production, but 0 is safer in Colab/scripts\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=params['MiniBatchSize'],\n",
        "        shuffle=False,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    # 2. Setup Device, Optimizer, Loss\n",
        "    model.to(torchdevice())\n",
        "\n",
        "    # Use Mean Squared Error (MSE) loss, common for regression/denoising tasks\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Optimizer: Adam is a good default replacement for MATLAB's 'adam' optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=params['InitialLearnRate'])\n",
        "\n",
        "    # 3. Learning Rate Scheduler\n",
        "    # LearnRateDropPeriod: 5, LearnRateDropFactor: 0.5\n",
        "    scheduler = PiecewiseScheduler(\n",
        "        optimizer,\n",
        "        step_size=params['LearnRateDropPeriod'],\n",
        "        factor=params['LearnRateDropFactor']\n",
        "    )\n",
        "\n",
        "    # Validation frequency (imgN is total training samples)\n",
        "    # The MATLAB ValidationFrequency is typically in iterations,\n",
        "    # often set to the total number of training samples (imgN).\n",
        "    # This means validation happens after seeing 'imgN' samples,\n",
        "    # which corresponds to `imgN / MiniBatchSize` iterations.\n",
        "    # We will trigger validation after a certain number of *batches*\n",
        "    # that accumulates to at least ValidationFrequency samples.\n",
        "    val_freq_batches = max(1, params['ValidationFrequency'] // params['MiniBatchSize'])\n",
        "\n",
        "\n",
        "    # 4. Training Loop\n",
        "    print(f\"Starting training for {params['MaxEpochs']} epochs on {torchdevice()}...\")\n",
        "\n",
        "    # Track best model based on validation loss\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(1, params['MaxEpochs'] + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        batches_processed_in_epoch = 0\n",
        "\n",
        "        # --- Training Step ---\n",
        "        for i, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs, targets = inputs.to(torchdevice()), targets.to(torchdevice())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            batches_processed_in_epoch += 1\n",
        "\n",
        "            # --- Validation (Training Iteration Check) ---\n",
        "            # Trigger validation based on the number of batches processed\n",
        "            if batches_processed_in_epoch % val_freq_batches == 0 or (i + 1) == len(train_loader):\n",
        "                 # Avoid validating multiple times in the last partial interval\n",
        "                 if (i + 1) < len(train_loader) and (i+1) % val_freq_batches != 0:\n",
        "                     continue\n",
        "\n",
        "                 avg_train_loss = running_loss / batches_processed_in_epoch\n",
        "\n",
        "                 # Validation Phase\n",
        "                 model.eval()\n",
        "                 val_loss = 0.0\n",
        "                 with torch.no_grad():\n",
        "                     for val_inputs, val_targets in val_loader:\n",
        "                         val_inputs, val_targets = val_inputs.to(torchdevice()), val_targets.to(torchdevice())\n",
        "                         val_outputs = model(val_inputs)\n",
        "                         val_loss += criterion(val_outputs, val_targets).item()\n",
        "\n",
        "                 avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "                 nowstr = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "                 print(f\"{nowstr} Epoch {epoch}/{params['MaxEpochs']}, Batch {i+1}/{len(train_loader)} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "                 # Save best model\n",
        "                 if avg_val_loss < best_val_loss:\n",
        "                     best_val_loss = avg_val_loss\n",
        "                     # Save the model state (weights)\n",
        "                     torch.save(model.state_dict(), f'{component_name}_best_model.pth')\n",
        "                     # print(f\"Saved best model with Val Loss: {best_val_loss:.6f}\")\n",
        "\n",
        "                 # Reset metrics for the next 'ValidationFrequency' interval within the epoch\n",
        "                 running_loss = 0.0\n",
        "                 batches_processed_in_epoch = 0\n",
        "                 model.train() # Switch back to training mode\n",
        "\n",
        "\n",
        "        # --- Scheduler Step (After each Epoch) ---\n",
        "        scheduler.step()\n",
        "\n",
        "    print(f\"\\n{component_name} Training Complete. Best Validation Loss: {best_val_loss:.6f}\")\n",
        "\n",
        "    # Load and return the best weights found during training\n",
        "    # Ensure the file exists before loading\n",
        "    model_save_path = f'{component_name}_best_model.pth'\n",
        "    if os.path.exists(model_save_path):\n",
        "        model.load_state_dict(torch.load(model_save_path))\n",
        "    else:\n",
        "        print(f\"Warning: Best model weights file '{model_save_path}' not found. Returning the model after the last epoch.\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 5. UTIRnet Reconstruction Function\n",
        "# =====================================================================\n",
        "\n",
        "def UTIRnetReconstruction(holoP, CNN_A, CNN_P, Z, lambda_val, dx, m1=0, pad_flag=1):\n",
        "    \"\"\"\n",
        "    Function for UTIRnet holographic reconstruction (MATLAB to Python conversion).\n",
        "    Args:\n",
        "        holoP (np.ndarray): Padded hologram intensity (2D array).\n",
        "        CNN_A (torch.nn.Module): Trained Amplitude CNN.\n",
        "        CNN_P (torch.nn.Module): Trained Phase CNN.\n",
        "        Z (float): Propagation distance (um).\n",
        "        lambda_val (float): Wavelength (um).\n",
        "        dx (float): Pixel size (um).\n",
        "        m1 (int): Display intermediate results flag (0: no, 1: yes).\n",
        "        pad_flag (int): Flag indicating if holoP is already padded (0: no, 1: yes).\n",
        "\n",
        "    Returns:\n",
        "        tuple: (Yout, Yamp, Yphs, Uout)\n",
        "            Yout (np.ndarray): Complex reconstruction (amplitude * exp(1j*phase)).\n",
        "            Yamp (np.ndarray): Amplitude reconstruction from CNN_A.\n",
        "            Yphs (np.ndarray): Phase reconstruction from CNN_P.\n",
        "            Uout (np.ndarray): Backpropagated complex field (with twin-image).\n",
        "    \"\"\"\n",
        "    # Constants\n",
        "    RI = 1 # refractive index of the medium (1 - air)\n",
        "    imS = 512 # image size (assuming reconstruction is done at original size)\n",
        "    ps = 256 # pad size (if padding is needed)\n",
        "\n",
        "    # Ensure the hologram is float64\n",
        "    holoP = holoP.astype(np.float64)\n",
        "\n",
        "    # Handle padding if pad_flag is 0\n",
        "    if pad_flag == 0:\n",
        "        holoP = padarray(holoP, (ps, ps), 'edge') # Pad the hologram\n",
        "\n",
        "    # Backpropagate hologram (intensity) to the object plane\n",
        "    # Uout is the complex field including the twin image\n",
        "    # The AS_propagate_p function expects complex input, so pass sqrt(intensity)\n",
        "    # and the function handles the propagation.\n",
        "    # Backpropagate intensity (abs(u)^2), not amplitude (abs(u)) for the twin image\n",
        "    # as per the original MATLAB code's use for input.\n",
        "    Uout = AS_propagate_p(holoP, Z, RI, lambda_val, dx)\n",
        "\n",
        "    if m1 == 1:\n",
        "        # Display input AS amplitude and phase\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "        im_amp = axes[0].imshow(np.abs(Uout), cmap='gray')\n",
        "        axes[0].set_title('Input AS amplitude (with twin-image)')\n",
        "        plt.colorbar(im_amp, ax=axes[0])\n",
        "        im_phs = axes[1].imshow(np.angle(Uout), cmap='gray')\n",
        "        axes[1].set_title('Input AS phase (with twin-image)')\n",
        "        plt.colorbar(im_phs, ax=axes[1])\n",
        "        for ax in axes: ax.axis('off'); ax.set_aspect('equal')\n",
        "        plt.suptitle('Intermediate AS Reconstruction')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    # Prepare input for CNNs: Extract amplitude and phase from Uout\n",
        "    # Convert numpy arrays to PyTorch tensors and add batch & channel dimensions\n",
        "    # Uout is complex (H, W). np.abs(Uout) is (H, W). Need (1, 1, H, W) for CNN.\n",
        "    input_amp = torch.from_numpy(np.abs(Uout)).unsqueeze(0).unsqueeze(0).float()\n",
        "    input_phs = torch.from_numpy(np.angle(Uout)).unsqueeze(0).unsqueeze(0).float()\n",
        "\n",
        "    # Move to device\n",
        "    input_amp = input_amp.to(torchdevice())\n",
        "    input_phs = input_phs.to(torchdevice())\n",
        "\n",
        "    # Set models to evaluation mode\n",
        "    CNN_A.eval()\n",
        "    CNN_P.eval()\n",
        "\n",
        "    # Run inference with torch.no_grad()\n",
        "    with torch.no_grad():\n",
        "        # Amplitude reconstruction\n",
        "        # CNN_A takes amplitude (1 channel) and outputs corrected amplitude (1 channel)\n",
        "        Yamp_tensor = CNN_A(input_amp)\n",
        "        # Phase reconstruction\n",
        "        # CNN_P takes phase (1 channel) and outputs corrected phase (1 channel)\n",
        "        Yphs_tensor = CNN_P(input_phs)\n",
        "\n",
        "    # Convert output tensors back to numpy arrays (remove batch and channel dimensions)\n",
        "    Yamp = Yamp_tensor.squeeze().cpu().numpy()\n",
        "    Yphs = Yphs_tensor.squeeze().cpu().numpy()\n",
        "\n",
        "    # Combine reconstructed amplitude and phase to get the complex field\n",
        "    # Yout = Yamp .* exp(1j*Yphs)\n",
        "    Yout = Yamp * np.exp(1j * Yphs)\n",
        "\n",
        "    # If padding was applied within this function, remove it before returning\n",
        "    if pad_flag == 0:\n",
        "         # Remove padding from Yout, Yamp, Yphs, and Uout\n",
        "         Yout = Yout[ps : ps + imS, ps : ps + imS]\n",
        "         Yamp = Yamp[ps : ps + imS, ps : ps + imS]\n",
        "         Yphs = Yphs[ps : ps + imS, ps : ps + imS]\n",
        "         # Uout already had padding removed in GenerateHologram if it was the source of holoP,\n",
        "         # but here it's backpropagated from the padded holoP, so it's also padded.\n",
        "         Uout = Uout[ps : ps + imS, ps : ps + imS]\n",
        "\n",
        "\n",
        "    return Yout, Yamp, Yphs, Uout\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 5. Execution\n",
        "# =====================================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # clear; close all; clc is handled by script execution environment\n",
        "\n",
        "    # System parameters ##########################\n",
        "    Z = 17000.0 # camera-sample distance (um)\n",
        "    lambda_val = 0.405 # light source wavelength (um)\n",
        "    dx = 2.4 # pixel size in object plane (cam_pix_size / mag) (um)\n",
        "\n",
        "    # Path to the directory containing flower recognition dataset\n",
        "    # (or placeholder for image data)\n",
        "    pth = '/content/drive/MyDrive/Colab Notebooks/SURF-2025-FALL/flowers/' # ############### CHANGE THIS PATH\n",
        "\n",
        "    # Mock Data Generation (Since the full dataset generation is time-consuming)\n",
        "    # Replace this with actual data loading from GenerateDataset if you have the data saved.\n",
        "    print(\"--- 1. Dataset generation (Mock Data) ---\")\n",
        "\n",
        "    # Create dummy data for training and validation\n",
        "    N_TRAIN = 500\n",
        "    N_VAL = 100\n",
        "    H, W = 512, 512\n",
        "\n",
        "    # Create random data with shape (N, H, W, 1)\n",
        "    inTrAmp = np.random.rand(N_TRAIN, H, W, 1).astype(np.float32)\n",
        "    tarTrAmp = np.random.rand(N_TRAIN, H, W, 1).astype(np.float32)\n",
        "    holosTrAmp = np.random.rand(N_TRAIN, H, W, 1).astype(np.float32)\n",
        "\n",
        "    inValAmp = np.random.rand(N_VAL, H, W, 1).astype(np.float32)\n",
        "    tarValAmp = np.random.rand(N_VAL, H, W, 1).astype(np.float32)\n",
        "    holosValAmp = np.random.rand(N_VAL, H, W, 1).astype(np.float32)\n",
        "\n",
        "    inTrPhs = np.random.rand(N_TRAIN, H, W, 1).astype(np.float32) * 2 * np.pi # Phase in [0, 2pi]\n",
        "    tarTrPhs = np.random.rand(N_TRAIN, H, W, 1).astype(np.float32) * 2 * np.pi\n",
        "    holosTrPhs = np.random.rand(N_TRAIN, H, W, 1).astype(np.float32)\n",
        "\n",
        "    inValPhs = np.random.rand(N_VAL, H, W, 1).astype(np.float32) * 2 * np.pi\n",
        "    tarValPhs = np.random.rand(N_VAL, H, W, 1).astype(np.float32) * 2 * np.pi\n",
        "    holosValPhs = np.random.rand(N_VAL, H, W, 1).astype(np.float32)\n",
        "\n",
        "    print(f\"Generated mock data: Train samples = {N_TRAIN}, Val samples = {N_VAL}\")\n",
        "\n",
        "\n",
        "    # Total training samples (used for ValidationFrequency)\n",
        "    imgN = N_TRAIN\n",
        "\n",
        "    # --- Parameters (Translated from MATLAB) ---\n",
        "    cnn_a_params = {\n",
        "        'MiniBatchSize': 1,\n",
        "        'MaxEpochs': 30,\n",
        "        'InitialLearnRate': 1e-4,\n",
        "        'LearnRateDropPeriod': 5, # Drop LR every 5 epochs\n",
        "        'LearnRateDropFactor': 0.5, # Drop LR by 50%\n",
        "        'ValidationFrequency': imgN # Validate after N_TRAIN samples seen\n",
        "    }\n",
        "\n",
        "    cnn_p_params = cnn_a_params.copy() # Phase uses the same schedule for this example\n",
        "\n",
        "    # --- Initialize Models ---\n",
        "    CNN_A = UTIRnetComponent()\n",
        "    CNN_P = UTIRnetComponent()\n",
        "\n",
        "    # --- Train CNN_A (Amplitude) ---\n",
        "    print(f\"Conceptual training setup for CNN_A (Amplitude) with {cnn_a_params['MaxEpochs']} epochs...\")\n",
        "    # Pass mock data to train_network\n",
        "    CNN_A = train_network(CNN_A, cnn_a_params, inTrAmp, tarTrAmp, inValAmp, tarValAmp, 'CNN_A')\n",
        "\n",
        "    # --- Train CNN_P (Phase) ---\n",
        "    print(f\"\\nConceptual training setup for CNN_P (Phase) with {cnn_p_params['MaxEpochs']} epochs...\")\n",
        "    # Pass mock data to train_network\n",
        "    CNN_P = train_network(CNN_P, cnn_p_params, inTrPhs, tarTrPhs, inValPhs, tarValPhs, 'CNN_P')\n",
        "\n",
        "    print(\"\\nFinal models (CNN_A and CNN_P) are trained and ready for use in UTIRnetReconstruction.\")\n",
        "\n",
        "    # System parameters for saving\n",
        "    UTIRnet_info = {\n",
        "        'Z_mm': Z / 1000,\n",
        "        'lambda_um': lambda_val,\n",
        "        'dx_um': dx\n",
        "    }\n",
        "\n",
        "    # Save network (PLACEHOLDER)\n",
        "    fnm = f\"UTIRnet_my_Z-{UTIRnet_info['Z_mm']}mm_dx-{dx}um_lambda-{lambda_val*1000}nm.pkl\"\n",
        "    # save(fnm, 'CNN_A', 'CNN_P', 'CNN_A_info', 'CNN_P_info', 'UTIRnet_info')\n",
        "    print(f\"Conceptual model saving to: {fnm}\")\n",
        "\n",
        "    # -----------------------------------------------------------------\n",
        "    # %% Network testing - on validation data\n",
        "    print(\"\\n--- 3. Network testing - validation data ---\")\n",
        "\n",
        "    # Ensure system parameters are the same as used for network training\n",
        "    Z = 17000.0\n",
        "    lambda_val = 0.405\n",
        "    dx = 2.4\n",
        "\n",
        "    # Use the first sample (index 0) from the validation set\n",
        "    imNo = 0\n",
        "    AmpPhs = 1 # 1 - amplitude data, 2 - phase data\n",
        "\n",
        "    if AmpPhs == 1:\n",
        "        # Amp data\n",
        "        GT = tarValAmp[imNo, :, :, 0] # ground truth target image\n",
        "        holo = holosValAmp[imNo, :, :, 0] # hologram\n",
        "    else: # AmpPhs == 2\n",
        "        # Phs data\n",
        "        GT = tarValPhs[imNo, :, :, 0] # ground truth target image\n",
        "        holo = holosValPhs[imNo, :, :, 0] # hologram\n",
        "\n",
        "    ps = 256 # pad size\n",
        "    # Use the UTIRnetReconstruction function with pad_flag=1 since mock data is not padded here\n",
        "    # Or pad the mock data before passing it. Let's pad it for consistency with the function's logic.\n",
        "    holoP = padarray(holo, (ps, ps), 'edge')\n",
        "\n",
        "    # Reconstruction\n",
        "    # m1 is an optional parameter, 0 is the flag for not displaying intermediate plots\n",
        "    Yout, Yamp, Yphs, Uout = UTIRnetReconstruction(holoP, CNN_A, CNN_P, Z, lambda_val, dx, m1=1, pad_flag=1)\n",
        "\n",
        "    # Remove padding (already handled by UTIRnetReconstruction if pad_flag was 0, but here it's 1, so remove manually)\n",
        "    # The output from UTIRnetReconstruction when pad_flag=1 is still padded Uout.\n",
        "    # Yout, Yamp, Yphs should be the same size as GT, which is not padded.\n",
        "\n",
        "    imS = 512 # image size\n",
        "    # Extract the center part corresponding to the original image size\n",
        "    Yout_unpad = Yout[ps : ps + imS, ps : ps + imS]\n",
        "    Uout_unpad = Uout[ps : ps + imS, ps : ps + imS]\n",
        "    Yamp_unpad = Yamp[ps : ps + imS, ps : ps + imS]\n",
        "    Yphs_unpad = Yphs[ps : ps + imS, ps : ps + imS]\n",
        "\n",
        "\n",
        "    # Display results (Matplotlib equivalent)\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    if AmpPhs == 1:\n",
        "        # Determine display range based on the unpadded data for clarity\n",
        "        all_amp_data = np.concatenate([np.abs(Uout_unpad).flatten(), np.abs(Yout_unpad).flatten(), GT.flatten()])\n",
        "        rng = [np.min(all_amp_data), np.max(all_amp_data)]\n",
        "\n",
        "        im1 = axes[0].imshow(np.abs(Uout_unpad), cmap='gray', vmin=rng[0], vmax=rng[1])\n",
        "        axes[0].set_title('input AS amplitude (with twin-image)')\n",
        "        plt.colorbar(im1, ax=axes[0])\n",
        "\n",
        "        im2 = axes[1].imshow(np.abs(Yout_unpad), cmap='gray', vmin=rng[0], vmax=rng[1])\n",
        "        axes[1].set_title('UTIRnet amplitude reconstruction')\n",
        "        plt.colorbar(im2, ax=axes[1])\n",
        "\n",
        "        im3 = axes[2].imshow(GT, cmap='gray', vmin=rng[0], vmax=rng[1])\n",
        "        axes[2].set_title('Ground truth amplitrude (without twin-image)')\n",
        "        plt.colorbar(im3, ax=axes[2])\n",
        "\n",
        "    else: # AmpPhs == 2\n",
        "        # Determine display range based on the unpadded data for clarity\n",
        "        # Note: Phase is usually displayed in [-pi, pi] or [0, 2pi] range.\n",
        "        # The GT phase is normalized to 0-2pi in GenerateHologram.\n",
        "        # AS phase (np.angle) is [-pi, pi]. CNN output phase is not restricted.\n",
        "        # For display consistency, let's normalize all to [-pi, pi] range for display.\n",
        "        def normalize_phase_display(phase_img):\n",
        "            return (np.angle(np.exp(1j * phase_img))) # Wrap to [-pi, pi]\n",
        "\n",
        "        Uout_phase_disp = normalize_phase_display(Uout_unpad)\n",
        "        Yout_phase_disp = normalize_phase_display(Yout_unpad)\n",
        "        GT_phase_disp = normalize_phase_display(GT) # Assuming GT was 0-2pi, wrap it.\n",
        "\n",
        "        rng = [-np.pi, np.pi]\n",
        "\n",
        "        im1 = axes[0].imshow(Uout_phase_disp, cmap='gray', vmin=rng[0], vmax=rng[1])\n",
        "        axes[0].set_title('input AS phase (with twin-image)')\n",
        "        plt.colorbar(im1, ax=axes[0])\n",
        "\n",
        "        im2 = axes[1].imshow(Yout_phase_disp, cmap='gray', vmin=rng[0], vmax=rng[1])\n",
        "        axes[1].set_title('UTIRnet reconstruction')\n",
        "        plt.colorbar(im2, ax=axes[1])\n",
        "\n",
        "        im3 = axes[2].imshow(GT_phase_disp, cmap='gray', vmin=rng[0], vmax=rng[1])\n",
        "        axes[2].set_title('Ground truth (without twin-image)')\n",
        "        plt.colorbar(im3, ax=axes[2])\n",
        "\n",
        "\n",
        "    for ax in axes:\n",
        "        ax.axis('off')\n",
        "        ax.set_aspect('equal')\n",
        "    plt.show()\n",
        "\n",
        "    # -----------------------------------------------------------------\n",
        "    # %% Network testing - generating and reconstructing synth data\n",
        "    print(\"\\n--- 4. Network testing - synthetic data ---\")\n",
        "\n",
        "    # Ensure system parameters are the same as used for network training\n",
        "    Z = 17000.0\n",
        "    lambda_val = 0.405\n",
        "    dx = 2.4\n",
        "\n",
        "    AmpPhs = 2 # 1 - amplitude data, 2 - phase data\n",
        "\n",
        "    # Read any image ####################################\n",
        "    # Use a dummy image since the file 'rice.png' is not available\n",
        "    # If you have 'rice.png' or another image, replace the path.\n",
        "    img_file_path = '/content/drive/MyDrive/Colab Notebooks/SURF-2025-FALL/flowers/daisy/10172379554_b2960e37f6_n.jpg' # ############### CHANGE THIS PATH or use a dummy image\n",
        "\n",
        "    try:\n",
        "        # Attempt to load a real image\n",
        "        img = np.asarray(Image.open(img_file_path).convert('L'))\n",
        "        print(f\"Loaded synthetic test image from: {img_file_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"WARNING: Image file not found at {img_file_path}. Using dummy image.\")\n",
        "        img = np.random.randint(0, 256, size=(512, 512), dtype=np.uint8) # Create a dummy image\n",
        "\n",
        "    if AmpPhs == 1:\n",
        "        input_img, GT, holo = GenerateHologram(img, Z, lambda_val, dx, 'amp')\n",
        "    else: # AmpPhs == 2\n",
        "        input_img, GT, holo = GenerateHologram(img, Z, lambda_val, dx, 'phs')\n",
        "\n",
        "    ps = 256\n",
        "    holoP = padarray(holo, (ps, ps), 'edge')\n",
        "\n",
        "    # Reconstruction\n",
        "    # Use UTIRnetReconstruction with pad_flag=1 as holoP is already padded\n",
        "    Yout, Yamp, Yphs, Uout = UTIRnetReconstruction(holoP, CNN_A, CNN_P, Z, lambda_val, dx, m1=1, pad_flag=1)\n",
        "\n",
        "    # Remove padding from the output (as done for validation data)\n",
        "    imS = 512 # image size\n",
        "    Yout_unpad = Yout[ps : ps + imS, ps : ps + imS]\n",
        "    Uout_unpad = Uout[ps : ps + imS, ps : ps + imS]\n",
        "    Yamp_unpad = Yamp[ps : ps + imS, ps : ps + imS]\n",
        "    Yphs_unpad = Yphs[ps : ps + imS, ps : ps + imS]\n",
        "\n",
        "\n",
        "    # Display results (Matplotlib equivalent)\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    if AmpPhs == 1:\n",
        "        # Determine display range based on the unpadded data for clarity\n",
        "        all_amp_data = np.concatenate([np.abs(Uout_unpad).flatten(), np.abs(Yout_unpad).flatten(), GT.flatten()])\n",
        "        rng = [np.min(all_amp_data), np.max(all_amp_data)]\n",
        "\n",
        "        im1 = axes[0].imshow(np.abs(Uout_unpad), cmap='gray', vmin=rng[0], vmax=rng[1])\n",
        "        axes[0].set_title('input AS amplitude (with twin-image)')\n",
        "        plt.colorbar(im1, ax=axes[0])\n",
        "\n",
        "        im2 = axes[1].imshow(np.abs(Yout_unpad), cmap='gray', vmin=rng[0], vmax=rng[1])\n",
        "        axes[1].set_title('UTIRnet amplitude reconstruction')\n",
        "        plt.colorbar(im2, ax=axes[1])\n",
        "\n",
        "        im3 = axes[2].imshow(GT, cmap='gray', vmin=rng[0], vmax=rng[1])\n",
        "        axes[2].set_title('Ground truth amplitrude (without twin-image)')\n",
        "        plt.colorbar(im3, ax=axes[2])\n",
        "\n",
        "    else: # AmpPhs == 2\n",
        "        # Normalize phases for display consistency\n",
        "        def normalize_phase_display(phase_img):\n",
        "            return (np.angle(np.exp(1j * phase_img))) # Wrap to [-pi, pi]\n",
        "\n",
        "        Uout_phase_disp = normalize_phase_display(Uout_unpad)\n",
        "        Yout_phase_disp = normalize_phase_display(Yout_unpad)\n",
        "        GT_phase_disp = normalize_phase_display(GT) # Assuming GT was 0-2pi, wrap it.\n",
        "\n",
        "\n",
        "        rng = [-np.pi, np.pi]\n",
        "\n",
        "        im1 = axes[0].imshow(Uout_phase_disp, cmap='gray', vmin=rng[0], vmax=rng[1])\n",
        "        axes[0].set_title('input AS phase (with twin-image)')\n",
        "        plt.colorbar(im1, ax=axes[0])\n",
        "\n",
        "        im2 = axes[1].imshow(Yout_phase_disp, cmap='gray', vmin=rng[0], vmax=rng[1])\n",
        "        axes[1].set_title('UTIRnet reconstruction')\n",
        "        plt.colorbar(im2, ax=axes[1])\n",
        "\n",
        "        im3 = axes[2].imshow(GT_phase_disp, cmap='gray', vmin=rng[0], vmax=rng[1])\n",
        "        axes[2].set_title('Ground truth (without twin-image)')\n",
        "        plt.colorbar(im3, ax=axes[2])\n",
        "\n",
        "    for ax in axes:\n",
        "        ax.axis('off')\n",
        "        ax.set_aspect('equal')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # -----------------------------------------------------------------\n",
        "    # %% Network testing - experimental data\n",
        "    print(\"\\n--- 5. Network testing - experimental data (PLACEHOLDER) ---\")\n",
        "\n",
        "    # PLACEHOLDER: Load hologram and network data\n",
        "    # NOTE: These files are not provided and must be replaced by actual data.\n",
        "\n",
        "    m1 = 1\n",
        "\n",
        "    # load hologram data %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "    # load('.\\Holograms\\CheekCells_17.08mm.mat')\n",
        "    # Mocking holo, Z_um, lambda_val, dx\n",
        "    # Ensure mock experimental data has the expected size (e.g., 512x512)\n",
        "    holo_exp = np.random.rand(512, 512).astype(np.float64) # Use float64 for consistency\n",
        "    Z_um = 17000.0\n",
        "    lambda_exp = 0.405\n",
        "    dx_exp = 2.4\n",
        "\n",
        "    # load network trained for propper system parameters %%%%%%%%%%%%%%%%%%%%%%\n",
        "    # load('.\\Networks\\UTIRnet_Z-17mm_dx-2.4um_l1ambda-405nm.mat')\n",
        "    # Assuming CNN_A and CNN_P were trained above and loaded from .pth files\n",
        "\n",
        "    # reconstruction\n",
        "    # Pass holo_exp directly (assuming it's the padded hologram or let the function pad)\n",
        "    # If holo_exp is the original size (512x512), set pad_flag=0 to enable padding inside\n",
        "    # If holo_exp is already padded (e.g., 1024x1024), set pad_flag=1 and ensure its size is handled.\n",
        "    # Let's assume holo_exp is original size and set pad_flag=0.\n",
        "    Yout, Yamp, Yphs, Uout = UTIRnetReconstruction(holo_exp,\n",
        "        CNN_A, CNN_P, Z_um, lambda_exp, dx_exp, m1=m1, pad_flag=0) # Set pad_flag=0\n",
        "\n",
        "    # Displaying (Yout, Yamp, Yphs, Uout from UTIRnetReconstruction will be unpadded if pad_flag=0)\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Amplitude plots\n",
        "    # Determine display range based on the unpadded data\n",
        "    all_exp_amp_data = np.concatenate([np.abs(Uout).flatten(), np.abs(Yout).flatten()])\n",
        "    rng_amp = [np.min(all_exp_amp_data), np.max(all_exp_amp_data)]\n",
        "\n",
        "    im1 = axes[0].imshow(np.abs(Uout), cmap='gray', vmin=rng_amp[0], vmax=rng_amp[1])\n",
        "    axes[0].set_title('AS amplitude (with twin-image)')\n",
        "    plt.colorbar(im1, ax=axes[0])\n",
        "\n",
        "    im2 = axes[1].imshow(np.abs(Yout), cmap='gray', vmin=rng_amp[0], vmax=rng_amp[1])\n",
        "    axes[1].set_title('UTIRnet amplitude reconstruction')\n",
        "    plt.colorbar(im2, ax=axes[1])\n",
        "\n",
        "    # Reference/Ground Truth placeholders\n",
        "    axes[2].set_title('Reference/Ground Truth (N/A)')\n",
        "    axes[2].text(0.5, 0.5, 'Reference/Ground Truth\\nData Not Loaded',\n",
        "                 horizontalalignment='center', verticalalignment='center',\n",
        "                 fontsize=12, color='red', transform=axes[2].transAxes)\n",
        "\n",
        "\n",
        "    # Phase plots\n",
        "    # Normalize phases for display consistency\n",
        "    def normalize_phase_display(phase_img):\n",
        "        return (np.angle(np.exp(1j * phase_img))) # Wrap to [-pi, pi]\n",
        "\n",
        "    Uout_phase_disp = normalize_phase_display(Uout)\n",
        "    Yout_phase_disp = normalize_phase_display(Yout)\n",
        "\n",
        "\n",
        "    rng_phs = [-np.pi, np.pi] # Display range for phase\n",
        "\n",
        "    im4 = axes[3].imshow(Uout_phase_disp, cmap='gray', vmin=rng_phs[0], vmax=rng_phs[1])\n",
        "    axes[3].set_title('AS phase (with twin-image)')\n",
        "    plt.colorbar(im4, ax=axes[3])\n",
        "\n",
        "    im5 = axes[4].imshow(Yout_phase_disp, cmap='gray', vmin=rng_phs[0], vmax=rng_phs[1])\n",
        "    axes[4].set_title('UTIRnet reconstruction')\n",
        "    plt.colorbar(im5, ax=axes[4])\n",
        "\n",
        "    # Reference/Ground Truth placeholders\n",
        "    axes[5].set_title('Reference/Ground Truth (N/A)')\n",
        "    axes[5].text(0.5, 0.5, 'Reference/Ground Truth\\nData Not Loaded',\n",
        "                 horizontalalignment='center', verticalalignment='center',\n",
        "                 fontsize=12, color='red', transform=axes[5].transAxes)\n",
        "\n",
        "\n",
        "    for ax in axes:\n",
        "        ax.axis('off')\n",
        "        ax.set_aspect('equal')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "cQ7TJA7F9vJN",
        "outputId": "6c05a090-1b44-439d-ac73-cb0c3f3a9b47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Dataset generation (Mock Data) ---\n",
            "Generated mock data: Train samples = 500, Val samples = 100\n",
            "Conceptual training setup for CNN_A (Amplitude) with 30 epochs...\n",
            "\n",
            "--- Training CNN_A ---\n",
            "Starting training for 30 epochs on cuda...\n",
            "2025-10-18 01:30:16 Epoch 1/30, Batch 500/500 | Train Loss: 0.088046 | Val Loss: 0.083355\n",
            "2025-10-18 01:33:31 Epoch 2/30, Batch 500/500 | Train Loss: 0.083347 | Val Loss: 0.083349\n",
            "2025-10-18 01:36:52 Epoch 3/30, Batch 500/500 | Train Loss: 0.083344 | Val Loss: 0.083350\n",
            "2025-10-18 01:40:23 Epoch 4/30, Batch 500/500 | Train Loss: 0.083343 | Val Loss: 0.083344\n",
            "2025-10-18 01:43:50 Epoch 5/30, Batch 500/500 | Train Loss: 0.083342 | Val Loss: 0.083343\n",
            "2025-10-18 01:46:05 Epoch 6/30, Batch 500/500 | Train Loss: 0.083341 | Val Loss: 0.083348\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 599\u001b[39m\n\u001b[32m    597\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConceptual training setup for CNN_A (Amplitude) with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcnn_a_params[\u001b[33m'\u001b[39m\u001b[33mMaxEpochs\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m epochs...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    598\u001b[39m \u001b[38;5;66;03m# Pass mock data to train_network\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m CNN_A = \u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCNN_A\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcnn_a_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minTrAmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarTrAmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minValAmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarValAmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCNN_A\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;66;03m# --- Train CNN_P (Phase) ---\u001b[39;00m\n\u001b[32m    602\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mConceptual training setup for CNN_P (Phase) with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcnn_p_params[\u001b[33m'\u001b[39m\u001b[33mMaxEpochs\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m epochs...\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 372\u001b[39m, in \u001b[36mtrain_network\u001b[39m\u001b[34m(model, params, inputs_train, targets_train, inputs_val, targets_val, component_name)\u001b[39m\n\u001b[32m    369\u001b[39m loss.backward()\n\u001b[32m    370\u001b[39m optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    373\u001b[39m batches_processed_in_epoch += \u001b[32m1\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;66;03m# --- Validation (Training Iteration Check) ---\u001b[39;00m\n\u001b[32m    376\u001b[39m \u001b[38;5;66;03m# Trigger validation based on the number of batches processed\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7S6BqLf2mp_1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}